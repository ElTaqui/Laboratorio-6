{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_network(input_shape, action_space):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(32, 8, strides=4, activation='relu'),\n",
    "        layers.Conv2D(64, 4, strides=2, activation='relu'),\n",
    "        layers.Conv2D(64, 3, strides=1, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(action_space, activation='linear')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_network, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return action_space.sample()  # Exploración\n",
    "    else:\n",
    "        q_values = q_network.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        return q_values.flatten()  # Explotación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(q_network, target_network, memory, gamma, batch_size, optimizer):\n",
    "    if len(memory) < batch_size:\n",
    "        return  # No entrenar hasta que haya suficiente memoria\n",
    "\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "    states = np.array(states)\n",
    "    next_states = np.array(next_states)\n",
    "    \n",
    "    q_values = q_network.predict(states, verbose=0)\n",
    "    q_values_next = target_network.predict(next_states, verbose=0)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        target_q = rewards[i]\n",
    "        if not dones[i]:\n",
    "            target_q += gamma * np.max(q_values_next[i])\n",
    "        q_values[i] = q_values[i].flatten()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values_pred = q_network(states)\n",
    "        loss = tf.reduce_mean(tf.square(q_values_pred - q_values))\n",
    "    grads = tape.gradient(loss, q_network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_to_start(state, start_position):\n",
    "    # Asume que `state` y `start_position` son posiciones 2D (x, y) del agente en la pista\n",
    "    return np.linalg.norm(np.array(state[:2]) - np.array(start_position))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes, max_steps=1000, batch_size=64, gamma=0.99, epsilon_decay=0.995, epsilon_min=0.1, render=False):\n",
    "    # Inicialización del entorno y de los parámetros de entrenamiento\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode='human')\n",
    "    input_shape = env.observation_space.shape\n",
    "    action_space = env.action_space\n",
    "    \n",
    "    # Inicialización de las redes Q y objetivo\n",
    "    q_network = create_q_network(input_shape, action_space.shape[0])\n",
    "    target_network = create_q_network(input_shape, action_space.shape[0])\n",
    "    target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "    # Definición del optimizador y parámetros de epsilon para exploración\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    epsilon = 1.0  # Comenzar con alta exploración\n",
    "    memory = deque(maxlen=100000)  # Memoria de experiencias\n",
    "    reward_per_episode = np.zeros(episodes)\n",
    "\n",
    "    # Parámetros adicionales para el cálculo de vueltas y recompensas\n",
    "    proximity_threshold = 10.0  # Distancia para considerar que el agente ha completado una vuelta\n",
    "    lap_reward = 100  # Recompensa adicional por completar una vuelta\n",
    "    slow_progress_penalty = -10  # Penalización por progreso lento\n",
    "    min_progress_distance = 5  # Distancia mínima que debe recorrer en cada intervalo\n",
    "\n",
    "    # Ciclo de entrenamiento a través de los episodios\n",
    "    for episode in range(episodes):\n",
    "        # Reiniciar el entorno y establecer el punto inicial\n",
    "        state, _ = env.reset(seed=42)\n",
    "        initial_position = state[:2]  # Guardar la posición inicial del agente\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        last_position = initial_position  # Posición del último progreso registrado\n",
    "        progress_interval = max_steps // 10  # Intervalo para verificar progreso\n",
    "\n",
    "        print(f\"Comenzando episodio {episode + 1}\")\n",
    "\n",
    "        while not done and step_count < max_steps:\n",
    "            # Selección de acción usando la política epsilon-greedy\n",
    "            action = choose_action(state, q_network, epsilon, action_space)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "\n",
    "            # Calcular la distancia actual al punto de inicio\n",
    "            distance_to_start = calculate_distance_to_start(next_state, initial_position)\n",
    "\n",
    "            # Penalización adicional si el agente no ha avanzado en cierta distancia\n",
    "            if step_count % progress_interval == 0:\n",
    "                distance_covered = calculate_distance_to_start(next_state, last_position)\n",
    "                if distance_covered < min_progress_distance:\n",
    "                    reward += slow_progress_penalty  # Penalización por progreso lento\n",
    "                    print(\"Penalización por progreso lento aplicada.\")\n",
    "                last_position = next_state[:2]  # Actualizar última posición de progreso\n",
    "\n",
    "            # Aplicar recompensa adicional si el agente completa una vuelta\n",
    "            if distance_to_start < proximity_threshold and step_count > max_steps // 2:\n",
    "                reward += lap_reward\n",
    "                print(\"¡Vuelta completada! Recompensa adicional otorgada.\")\n",
    "            \n",
    "            # Almacenar la transición en la memoria\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Entrenamiento de la red Q usando experiencias almacenadas en la memoria\n",
    "            train_network(q_network, target_network, memory, gamma, batch_size, optimizer)\n",
    "\n",
    "            # Finalizar el episodio si la recompensa acumulada es muy negativa\n",
    "            if episode_reward < -100:\n",
    "                done = True\n",
    "                print(f\"Episodio {episode + 1} terminado anticipadamente por baja recompensa acumulada.\")\n",
    "        \n",
    "        # Reducir epsilon después de cada episodio para disminuir exploración gradualmente\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        # Almacenar la recompensa total del episodio\n",
    "        reward_per_episode[episode] = episode_reward\n",
    "\n",
    "        # Sincronizar la red objetivo periódicamente\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "        # Mostrar progreso cada 100 episodios\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episodio: {episode + 1} - Recompensa acumulada: {reward_per_episode[episode]}\")\n",
    "\n",
    "    # Graficar la recompensa acumulada por episodio al finalizar el entrenamiento\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel(\"Episodios\")\n",
    "    plt.ylabel(\"Recompensa acumulada\")\n",
    "    plt.title(\"Desempeño del agente durante el entrenamiento\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluación del agente entrenado\n",
    "    evaluate_agent(env, q_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, q_network, max_steps=1000):\n",
    "    state, _ = env.reset(seed=42)\n",
    "    state = np.array(state, dtype=np.float32)\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done and step_count < max_steps:\n",
    "        action = q_network.predict(np.expand_dims(state, axis=0), verbose=0).flatten()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Recompensa total en evaluación: {total_reward}\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando episodio 1\n",
      "Comenzando episodio 2\n",
      "Comenzando episodio 3\n",
      "Comenzando episodio 4\n",
      "Comenzando episodio 5\n",
      "Comenzando episodio 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29088\\3336537643.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29088\\2940247101.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(episodes, max_steps, batch_size, gamma, epsilon_decay, epsilon_min, render)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mstep_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;31m# Entrenamiento de la red Q usando experiencias almacenadas en la memoria\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m# Finalizar el episodio si la recompensa acumulada es muy negativa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepisode_reward\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29088\\207089127.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(q_network, target_network, memory, gamma, batch_size, optimizer)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mq_values_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values_pred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1062\u001b[0m               \u001b[0moutput_gradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1064\u001b[0m                           \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_gradients\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \u001b[1;34m\"Unknown value for unconnected_gradients: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0munconnected_gradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    590\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0m\u001b[0;32m    595\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m           \u001b[0mshape_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1631\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dilations\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1635\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1636\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m       return conv2d_backprop_filter_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(episodes=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
